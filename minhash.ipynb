{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code belows performs basic MinHash near-deduplication without any locality sensitive hashing (lsh). It simply computes minhash signatures for each document, and finds pairs of documents with high overlap of minhashes in their signatures, removing one of them."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from xxhash import xxh64_intdigest\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    id: int\n",
    "    text: str\n",
    "\n",
    "def ngrams(sequence: list, n: int):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a sequence of items\n",
    "    Example:\n",
    "        ngrams(\"Hi how are you?\".split(), 3) -> [('Hi', 'how', 'are'), ('how', 'are', 'you')]\n",
    "    \"\"\"\n",
    "    if len(sequence) < n:\n",
    "        return []\n",
    "    \n",
    "    return [tuple(sequence[i:i+n]) for i in range(len(sequence) - n + 1)]\n",
    "\n",
    "def get_hash(text: str, seed: int) -> int:\n",
    "    \"\"\"\n",
    "    Get the hash of a text using a seed\n",
    "    \"\"\"\n",
    "    return xxh64_intdigest(text, seed)\n",
    "\n",
    "def get_signatures(shingles: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get signatures (minhash of n-grams) from a string of text\n",
    "\n",
    "    Args:\n",
    "        shingles: numpy array of shingles: dtype = uint64, shape = (k, n_grams)\n",
    "\n",
    "    Returns:\n",
    "        numpy array of signatures: dtype = uint64, shape = (k, n_grams)\n",
    "    \"\"\"\n",
    "    return np.min(shingles, axis=1)\n",
    "\n",
    "def get_shingles(text: str, n_grams: int, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get kxn shingles (hashed n-grams) from a string of text\n",
    "\n",
    "    Args:\n",
    "        text: input text\n",
    "        n_grams: n-grams size to use\n",
    "        k: number of hash functions to use\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shingles: dtype = uint64, shape = (k, n_grams)\n",
    "    \"\"\"\n",
    "    text_ngrams = ngrams(text.split(), n_grams)\n",
    "    ngrams_hashes = np.array([\n",
    "        [\n",
    "            # for each hash function(seed) compute the hash of each text_ngram\n",
    "            xxh64_intdigest(\" \".join(text_ngram), seed)\n",
    "            for text_ngram in text_ngrams\n",
    "        ] for seed in range(k)\n",
    "    ], dtype=np.uint64)\n",
    "    return ngrams_hashes\n",
    "\n",
    "\n",
    "\n",
    "def dedup(data: list[Document], n_grams: int, k: int, jaccard_threshold: float = 0.8):\n",
    "    \"\"\"\n",
    "        Takes a list of documents and near-deduplicates them using minhash with `n_grams`-grams, `k` hashes per document and a minimum jaccard similarity of `jaccard_threshold` to remove documents\n",
    "    :param data: list of documents\n",
    "    :param n_grams: size of n-grams to consider. for example 3 will consider contiguous 3 word-grams\n",
    "    :param k: number of hash functions to use\n",
    "    :param jaccard_threshold: minimum threshold to consider 2 documents as duplicates and remove one of them\n",
    "    :return: a subset of `data` without near-duplicates\n",
    "    \"\"\"\n",
    "    # First stage: create a signatures (k minhashes of n-grams) for each document\n",
    "    signatures: list[tuple[int, np.ndarray]] = []\n",
    "    for doc in data:\n",
    "        shingles = get_shingles(doc.text, n_grams, k)\n",
    "        if shingles.size != 0:\n",
    "            signatures.append((doc.id, get_signatures(shingles)))\n",
    "    \n",
    "\n",
    "    # Second stage: compute the jaccard similarity between all signature pairs\n",
    "    # When duplicates are found, always keep only the one with the smallest index\n",
    "    to_remove_ids = set()\n",
    "    for i, (doc_id_i, sig_i) in enumerate(signatures):\n",
    "        for doc_id_j, sig_j in signatures[i+1:]:\n",
    "            # ratio of hashes that match between the 2 signatures\n",
    "            jaccard_similarity = np.sum(sig_i == sig_j) / len(sig_i)\n",
    "            # if above the minimum threshold, we consider them as duplicates and mark one for removal\n",
    "            if jaccard_similarity > jaccard_threshold:\n",
    "                to_remove_ids.add(doc_id_j)\n",
    "\n",
    "    # We iterate through the data and only keep the ones that are not in to_remove_indices\n",
    "    kept_docs = []\n",
    "    for doc in data:\n",
    "        if doc.id not in to_remove_ids:\n",
    "            kept_docs.append(doc)\n",
    "\n",
    "    return kept_docs\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_document(path: Path) -> str:\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "documents = [\n",
    "    Document(i, load_document(path)) for i, path in enumerate(Path(\"documents\").glob(\"*.txt\"))\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for doc in documents:\n",
    "    print(f\"Document {doc.id}: {doc.text}\")\n",
    "    print(\"-------\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "deduplicated_documents = dedup(data=documents, n_grams=5, k=10, jaccard_threshold=0.7)\n",
    "print(f\"Kept: {len(deduplicated_documents)}/{len(documents)} documents\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check the list of deduplicated documents. Anything that jumps out at you?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Kept documents:\")\n",
    "for doc in deduplicated_documents:\n",
    "    print(f\"Document {doc.id}: {doc.text}\")\n",
    "    print(\"-------\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
