{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code belows performs basic MinHash near-deduplication without any locality sensitive hashing (lsh). It simply computes minhash signatures for each document, and finds pairs of documents with high overlap of minhashes in their signatures, removing one of them.\n",
    "\n",
    "\n",
    "## Similarity and Minhash Overview\n",
    "### Similarity\n",
    "For documents A and B, we compute their similarity as (the number of n-grams that are in intersection between A and B) / (the number of n-grams that are in union between A and B). This is sometimes called the Jaccard similarity.\n",
    "\n",
    "This might not be always good (can you tell us why?), so instead minhash is used to approximate the Jaccard similarity.\n",
    "\n",
    "### Minhash\n",
    "To approximate the Jaccard similarity using Minhash, we first obtain minimal hash value of all n-grams in a document -> minhash_i(A).\n",
    "It's easy to see that P(minhash_i(A) == minhash_i(B)) is equal to the Jaccard similarity between A and B. To approximate this probability, we therfore use multiple independent hash functions and check how many of them match between A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xxhash numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from xxhash import xxh64_intdigest\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    id: int\n",
    "    text: str\n",
    "\n",
    "def ngrams(sequence: list, n: int):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a sequence of items\n",
    "    Example:\n",
    "        ngrams(\"Hi how are you?\".split(), 3) -> [('Hi', 'how', 'are'), ('how', 'are', 'you')]\n",
    "    \"\"\"\n",
    "    if len(sequence) < n:\n",
    "        return []\n",
    "    \n",
    "    return [tuple(sequence[i:i+n]) for i in range(len(sequence) - n + 1)]\n",
    "\n",
    "def get_signatures(shingles: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get signatures (minhash of n-grams) from a string of text\n",
    "\n",
    "    Args:\n",
    "        shingles: numpy array of shingles: dtype = uint64, shape = (k, n_grams)\n",
    "\n",
    "    Returns:\n",
    "        numpy array of signatures: dtype = uint64, shape = (k, n_grams)\n",
    "    \"\"\"\n",
    "    return np.min(shingles, axis=1)\n",
    "\n",
    "def get_shingles(text: str, n_grams: int, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get kxn shingles (hashed n-grams) from a string of text\n",
    "\n",
    "    Args:\n",
    "        text: input text\n",
    "        n_grams: n-grams size to use\n",
    "        k: number of hash functions to use\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shingles: dtype = uint64, shape = (k, n_grams)\n",
    "    \"\"\"\n",
    "    text_ngrams = ngrams(text.split(), n_grams)\n",
    "    ngrams_hashes = np.array([\n",
    "        [\n",
    "            # for each hash function(seed) compute the hash of each text_ngram\n",
    "            xxh64_intdigest(\" \".join(text_ngram), seed)\n",
    "            for text_ngram in text_ngrams\n",
    "        ] for seed in range(k)\n",
    "    ], dtype=np.uint64)\n",
    "    return ngrams_hashes\n",
    "\n",
    "\n",
    "\n",
    "def dedup(data: list[Document], n_grams: int, k: int, jaccard_threshold: float = 0.8):\n",
    "    \"\"\"\n",
    "        Takes a list of documents and near-deduplicates them using minhash with `n_grams`-grams, `k` hashes per document and a minimum jaccard similarity of `jaccard_threshold` to remove documents\n",
    "    :param data: list of documents\n",
    "    :param n_grams: size of n-grams to consider. for example 3 will consider contiguous 3 word-grams\n",
    "    :param k: number of hash functions to use\n",
    "    :param jaccard_threshold: minimum threshold to consider 2 documents as duplicates and remove one of them\n",
    "    :return: a subset of `data` without near-duplicates\n",
    "    \"\"\"\n",
    "    # First stage: create a signatures (k minhashes of n-grams) for each document\n",
    "    signatures: list[tuple[int, np.ndarray]] = []\n",
    "    for doc in data:\n",
    "        shingles = get_shingles(doc.text, n_grams, k)\n",
    "        if shingles.size != 0:\n",
    "            signatures.append((doc.id, get_signatures(shingles)))\n",
    "    \n",
    "\n",
    "    # Second stage: compute the jaccard similarity between all signature pairs\n",
    "    # When duplicates are found, always keep only the one with the smallest index\n",
    "    to_remove_ids = set()\n",
    "    for i, (doc_id_i, sig_i) in enumerate(signatures):\n",
    "        for doc_id_j, sig_j in signatures[i+1:]:\n",
    "            jaccard_similarity = np.sum(sig_i == sig_j) / len(sig_i)\n",
    "            if jaccard_similarity > jaccard_threshold:\n",
    "                to_remove_ids.add(doc_id_j)\n",
    "\n",
    "    # Third stage: remove the documents that are marked for removal\n",
    "    kept_docs = []\n",
    "    for doc in data:\n",
    "        if doc.id not in to_remove_ids:\n",
    "            kept_docs.append(doc)\n",
    "\n",
    "    return kept_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(path: Path) -> str:\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "documents = [\n",
    "    Document(i, load_document(path)) for i, path in enumerate(Path(\"documents\").glob(\"*.txt\"))\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    print(f\"Document {doc.id}: {doc.text}\")\n",
    "    print(\"-------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_documents = dedup(data=documents, n_grams=5, k=10, jaccard_threshold=0.7)\n",
    "print(f\"Kept: {len(deduplicated_documents)}/{len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the list of deduplicated documents. Anything that jumps out at you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kept documents:\")\n",
    "for doc in deduplicated_documents:\n",
    "    print(f\"Document {doc.id}: {doc.text}\")\n",
    "    print(\"-------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
