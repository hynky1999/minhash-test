{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf95eab23935317",
   "metadata": {},
   "source": [
    "Load documents from disk in 3 languages: russian, french, thai"
   ]
  },
  {
   "cell_type": "code",
   "id": "945a4c2848d8ab4d",
   "metadata": {},
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    id: int\n",
    "    text: str\n",
    "    remove_reason: str | None = None\n",
    "\n",
    "def load_documents(path: Path) -> list[Document]:\n",
    "    docs = []\n",
    "    for i, path in enumerate(sorted(path.glob(\"*.txt\"))):\n",
    "        with open(path, \"r\") as f:\n",
    "            docs.append(Document(i, f.read()))\n",
    "    return docs\n",
    "\n",
    "def save_documents(docs: list[Document], path: Path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for doc in docs:\n",
    "        with open(path / f\"{doc.id:03d}.txt\", \"w\") as f:\n",
    "            f.write(doc.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79dc98aa05510c62",
   "metadata": {},
   "source": [
    "LANGS = [\"rus_Cyrl\", \"fra_Latn\", \"tha_Thai\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef40bb7630b507a5",
   "metadata": {},
   "source": [
    "docs_per_lang = {\n",
    "    lang: load_documents(Path(f\"ml-documents/{lang}\")) for lang in LANGS\n",
    "}\n",
    "for lang, docs in docs_per_lang.items():\n",
    "    print(f\"Loaded {len(docs)} documents for {lang}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f70697d91b015b6",
   "metadata": {},
   "source": [
    "Let's define the filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "acd5723e1eda62e2",
   "metadata": {},
   "source": [
    "TERMINAL_PUNCTUATION = (\n",
    "    \"᪩\",\n",
    "    \"？\",\n",
    "    \"⁈\",\n",
    "    \"𑩂\",\n",
    "    \"．\",\n",
    "    \"꩞\",\n",
    "    \"𑅃\",\n",
    "    \"﹗\",\n",
    "    \"𑂾\",\n",
    "    \"\\u1b7d\",\n",
    "    \"፧\",\n",
    "    \"𑅂\",\n",
    "    \"꡶\",\n",
    "    \"꘎\",\n",
    "    \"⁉\",\n",
    "    \"࠾\",\n",
    "    \"᪨\",\n",
    "    \"𑊩\",\n",
    "    \"𑱂\",\n",
    "    \"᱿\",\n",
    "    \"𖩮\",\n",
    "    \"᥅\",\n",
    "    \"\\U00011f43\",\n",
    "    \"\\U00011f44\",\n",
    "    \"﹒\",\n",
    "    \"𑈹\",\n",
    "    \"𑈸\",\n",
    "    \"።\",\n",
    "    \"܂\",\n",
    "    \"؞\",\n",
    "    \"꛳\",\n",
    "    \"\\U00010f88\",\n",
    "    \"𑗍\",\n",
    "    \"𐩖\",\n",
    "    \"𑙂\",\n",
    "    \"\\u061d\",\n",
    "    \"꩟\",\n",
    "    \"᠉\",\n",
    "    \"\\u1b7e\",\n",
    "    \"𑗗\",\n",
    "    \"᰼\",\n",
    "    \"𑻸\",\n",
    "    \"؟\",\n",
    "    \"𑪜\",\n",
    "    \"꧉\",\n",
    "    \"𑗉\",\n",
    "    \"𐽙\",\n",
    "    \"𖫵\",\n",
    "    \"𖬷\",\n",
    "    \"܀\",\n",
    "    \"꓿\",\n",
    "    \"᜵\",\n",
    "    \"𑗏\",\n",
    "    \"𑁇\",\n",
    "    \"𑗓\",\n",
    "    \"𑥄\",\n",
    "    \"៖\",\n",
    "    \"𑥆\",\n",
    "    \"𑗑\",\n",
    "    \"𑗒\",\n",
    "    \"꯫\",\n",
    "    \"۔\",\n",
    "    \"𐩗\",\n",
    "    \"\\U00010f86\",\n",
    "    \"꡷\",\n",
    "    \"\\u2e54\",\n",
    "    \"｡\",\n",
    "    \"៕\",\n",
    "    \"߹\",\n",
    "    \"⸮\",\n",
    "    \".\",\n",
    "    \"𑇅\",\n",
    "    \"࠹\",\n",
    "    \"𛲟\",\n",
    "    \"꫰\",\n",
    "    \"꤯\",\n",
    "    \"𐽗\",\n",
    "    \"᭞\",\n",
    "    \"𑜼\",\n",
    "    \"፨\",\n",
    "    \"𑃁\",\n",
    "    \"꣏\",\n",
    "    \"𑇟\",\n",
    "    \"𖬸\",\n",
    "    \"𑪛\",\n",
    "    \"𑜾\",\n",
    "    \"࠷\",\n",
    "    \"𝪈\",\n",
    "    \"?\",\n",
    "    \"𑃀\",\n",
    "    \"𑗃\",\n",
    "    \"！\",\n",
    "    \"։\",\n",
    "    \"꣎\",\n",
    "    \"॥\",\n",
    "    \"𑗖\",\n",
    "    \"᭛\",\n",
    "    \"᠃\",\n",
    "    \"!\",\n",
    "    \"၊\",\n",
    "    \"𖺘\",\n",
    "    \"⁇\",\n",
    "    \"𑗌\",\n",
    "    \"𑑋\",\n",
    "    \"𖭄\",\n",
    "    \"᭟\",\n",
    "    \"𑅁\",\n",
    "    \"𑙁\",\n",
    "    \"⸼\",\n",
    "    \"꩝\",\n",
    "    \"𑗋\",\n",
    "    \"。\",\n",
    "    \"꧈\",\n",
    "    \"꫱\",\n",
    "    \"𑜽\",\n",
    "    \"𐽖\",\n",
    "    \"𑂿\",\n",
    "    \"᙮\",\n",
    "    \"។\",\n",
    "    \"꛷\",\n",
    "    \"\\U00010f89\",\n",
    "    \"៚\",\n",
    "    \"᥄\",\n",
    "    \"𑗕\",\n",
    "    \"𑗎\",\n",
    "    \"᪪\",\n",
    "    \"᭚\",\n",
    "    \"࠽\",\n",
    "    \"𑇞\",\n",
    "    \"𑗊\",\n",
    "    \"𐽘\",\n",
    "    \"\\u2e53\",\n",
    "    \"𑗔\",\n",
    "    \"𖩯\",\n",
    "    \"𑇍\",\n",
    "    \"𑻷\",\n",
    "    \"𐽕\",\n",
    "    \"𑩃\",\n",
    "    \"।\",\n",
    "    \"𑗂\",\n",
    "    \"𑇆\",\n",
    "    \"𑁈\",\n",
    "    \"။\",\n",
    "    \"᱾\",\n",
    "    \"𑱁\",\n",
    "    \"꘏\",\n",
    "    \"܁\",\n",
    "    \"᜶\",\n",
    "    \"‼\",\n",
    "    \"𑈻\",\n",
    "    \"‽\",\n",
    "    \"᪫\",\n",
    "    \"﹖\",\n",
    "    \"𑑌\",\n",
    "    \"𑈼\",\n",
    "    \"\\U00010f87\",\n",
    "    \"𑗐\",\n",
    "    \"៙\",\n",
    "    \"᰻\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "232bed2e4faae257",
   "metadata": {},
   "source": [
    "!pip install pythainlp pyyaml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41ed251aea5e8889",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def separate_words(lang: str, text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "        Separates string (text) in lang `lang` into multiple words\n",
    "    :param lang: \n",
    "    :param text: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if lang == \"tha_Thai\":\n",
    "        from pythainlp.tokenize import word_tokenize as th_word_tokenize\n",
    "\n",
    "        tokens = th_word_tokenize(text, keep_whitespace=False, engine=\"newmm-safe\")\n",
    "        return [el.strip() for el in tokens if len(el.strip()) > 0]\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def filter_docs(lang: str, docs: list[Document], filter_config: dict) -> tuple[list[Document], list[Document]]:\n",
    "    \"\"\"\n",
    "        Filters `docs` (that are in `lang` language) using some heuristic filters. Thresholds are defined in `filter_config`.\n",
    "        Returns (kept documents, removed documents)\n",
    "    :param lang: \n",
    "    :param docs: \n",
    "    :param filter_config: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    filtered, removed = [], []\n",
    "    for doc in docs:\n",
    "        lines = doc.text.split(\"\\n\")\n",
    "        lines = [line for line in lines if line.strip() != \"\"]\n",
    "        words = separate_words(lang, doc.text)\n",
    "        \n",
    "        if len(lines) == 0:\n",
    "            doc.remove_reason = \"empty\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "        \n",
    "        avg_n_words = np.mean([len(w) for w in words])\n",
    "        if avg_n_words <= filter_config[\"min_avg_word_length\"]:\n",
    "            doc.remove_reason = \"min_avg_word_length\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "\n",
    "        if avg_n_words >= filter_config[\"max_avg_word_length\"]:\n",
    "            doc.remove_reason = \"max_avg_word_length\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "            \n",
    "        if sum(w in filter_config[\"stopwords\"] for w in words) < 2:\n",
    "            doc.remove_reason = \"stopwords\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "            \n",
    "        ratio = sum(1 for line in lines if line.endswith(TERMINAL_PUNCTUATION)) / len(lines)\n",
    "        if ratio <= filter_config[\"line_punct_thr\"]:\n",
    "            doc.remove_reason = \"line_punct_thr\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "        \n",
    "        new_line = doc.text.count(\"\\n\")\n",
    "        if new_line / len(words) >= filter_config[\"new_line_ratio\"]:\n",
    "            doc.remove_reason = \"new_line_ratio\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "        \n",
    "        # all good\n",
    "        filtered.append(doc)\n",
    "    return filtered, removed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c360c5a639a207",
   "metadata": {},
   "source": [
    "Load thresholds from config"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb16d0a4041ecb55",
   "metadata": {},
   "source": [
    "import yaml\n",
    "\n",
    "filter_configs = {}\n",
    "for lang in LANGS:\n",
    "    with open(f\"filter-configs/{lang}.yml\") as f:\n",
    "        filter_configs[lang] = yaml.safe_load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94efdeadb501c7de",
   "metadata": {},
   "source": [
    "filter_configs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e30e8bb439ac7c3",
   "metadata": {},
   "source": [
    "Actually filter"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e83849a9628daf8",
   "metadata": {},
   "source": [
    "result = {}\n",
    "for lang, docs in docs_per_lang.items():\n",
    "    filter_config = filter_configs[lang]\n",
    "    filtered, removed = filter_docs(lang, docs, filter_config)\n",
    "    result[lang] = (filtered, removed)\n",
    "    save_documents(filtered, Path(f\"filter-results/{lang}/filtered\"))\n",
    "    save_documents(removed, Path(f\"filter-results/{lang}/removed\"))\n",
    "    print(f\"Filtered {len(docs)} {lang} documents. Kept: {len(filtered)} | Removed: {len(removed)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c28f729ce74b99d0",
   "metadata": {},
   "source": [
    "Some stats on the filtering"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fdaa99f38f0f087",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "for lang in LANGS:\n",
    "    reasons = Counter()\n",
    "    for removed_doc in result[lang][1]:\n",
    "        reasons[removed_doc.remove_reason] += 1\n",
    "    print(lang, \"remove reasons:\", reasons)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
