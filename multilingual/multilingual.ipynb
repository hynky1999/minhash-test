{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf95eab23935317",
   "metadata": {},
   "source": [
    "Load documents from disk in 3 languages: russian, french, thai"
   ]
  },
  {
   "cell_type": "code",
   "id": "945a4c2848d8ab4d",
   "metadata": {},
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    id: int\n",
    "    text: str\n",
    "    remove_reason: str | None = None\n",
    "\n",
    "def load_documents(path: Path) -> list[Document]:\n",
    "    docs = []\n",
    "    for i, path in enumerate(sorted(path.glob(\"*.txt\"))):\n",
    "        with open(path, \"r\") as f:\n",
    "            docs.append(Document(i, f.read()))\n",
    "    return docs\n",
    "\n",
    "def save_documents(docs: list[Document], path: Path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for doc in docs:\n",
    "        with open(path / f\"{doc.id:03d}.txt\", \"w\") as f:\n",
    "            f.write(doc.text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79dc98aa05510c62",
   "metadata": {},
   "source": [
    "LANGS = [\"rus_Cyrl\", \"fra_Latn\", \"tha_Thai\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef40bb7630b507a5",
   "metadata": {},
   "source": [
    "docs_per_lang = {\n",
    "    lang: load_documents(Path(f\"ml-documents/{lang}\")) for lang in LANGS\n",
    "}\n",
    "for lang, docs in docs_per_lang.items():\n",
    "    print(f\"Loaded {len(docs)} documents for {lang}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f70697d91b015b6",
   "metadata": {},
   "source": [
    "Let's define the filtering functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "acd5723e1eda62e2",
   "metadata": {},
   "source": [
    "TERMINAL_PUNCTUATION = (\n",
    "    \"áª©\",\n",
    "    \"ï¼Ÿ\",\n",
    "    \"âˆ\",\n",
    "    \"ð‘©‚\",\n",
    "    \"ï¼Ž\",\n",
    "    \"ê©ž\",\n",
    "    \"ð‘…ƒ\",\n",
    "    \"ï¹—\",\n",
    "    \"ð‘‚¾\",\n",
    "    \"\\u1b7d\",\n",
    "    \"á§\",\n",
    "    \"ð‘…‚\",\n",
    "    \"ê¡¶\",\n",
    "    \"ê˜Ž\",\n",
    "    \"â‰\",\n",
    "    \"à ¾\",\n",
    "    \"áª¨\",\n",
    "    \"ð‘Š©\",\n",
    "    \"ð‘±‚\",\n",
    "    \"á±¿\",\n",
    "    \"ð–©®\",\n",
    "    \"á¥…\",\n",
    "    \"\\U00011f43\",\n",
    "    \"\\U00011f44\",\n",
    "    \"ï¹’\",\n",
    "    \"ð‘ˆ¹\",\n",
    "    \"ð‘ˆ¸\",\n",
    "    \"á¢\",\n",
    "    \"Ü‚\",\n",
    "    \"Øž\",\n",
    "    \"ê›³\",\n",
    "    \"\\U00010f88\",\n",
    "    \"ð‘—\",\n",
    "    \"ð©–\",\n",
    "    \"ð‘™‚\",\n",
    "    \"\\u061d\",\n",
    "    \"ê©Ÿ\",\n",
    "    \"á ‰\",\n",
    "    \"\\u1b7e\",\n",
    "    \"ð‘——\",\n",
    "    \"á°¼\",\n",
    "    \"ð‘»¸\",\n",
    "    \"ØŸ\",\n",
    "    \"ð‘ªœ\",\n",
    "    \"ê§‰\",\n",
    "    \"ð‘—‰\",\n",
    "    \"ð½™\",\n",
    "    \"ð–«µ\",\n",
    "    \"ð–¬·\",\n",
    "    \"Ü€\",\n",
    "    \"ê“¿\",\n",
    "    \"áœµ\",\n",
    "    \"ð‘—\",\n",
    "    \"ð‘‡\",\n",
    "    \"ð‘—“\",\n",
    "    \"ð‘¥„\",\n",
    "    \"áŸ–\",\n",
    "    \"ð‘¥†\",\n",
    "    \"ð‘—‘\",\n",
    "    \"ð‘—’\",\n",
    "    \"ê¯«\",\n",
    "    \"Û”\",\n",
    "    \"ð©—\",\n",
    "    \"\\U00010f86\",\n",
    "    \"ê¡·\",\n",
    "    \"\\u2e54\",\n",
    "    \"ï½¡\",\n",
    "    \"áŸ•\",\n",
    "    \"ß¹\",\n",
    "    \"â¸®\",\n",
    "    \".\",\n",
    "    \"ð‘‡…\",\n",
    "    \"à ¹\",\n",
    "    \"ð›²Ÿ\",\n",
    "    \"ê«°\",\n",
    "    \"ê¤¯\",\n",
    "    \"ð½—\",\n",
    "    \"á­ž\",\n",
    "    \"ð‘œ¼\",\n",
    "    \"á¨\",\n",
    "    \"ð‘ƒ\",\n",
    "    \"ê£\",\n",
    "    \"ð‘‡Ÿ\",\n",
    "    \"ð–¬¸\",\n",
    "    \"ð‘ª›\",\n",
    "    \"ð‘œ¾\",\n",
    "    \"à ·\",\n",
    "    \"ðªˆ\",\n",
    "    \"?\",\n",
    "    \"ð‘ƒ€\",\n",
    "    \"ð‘—ƒ\",\n",
    "    \"ï¼\",\n",
    "    \"Ö‰\",\n",
    "    \"ê£Ž\",\n",
    "    \"à¥¥\",\n",
    "    \"ð‘—–\",\n",
    "    \"á­›\",\n",
    "    \"á ƒ\",\n",
    "    \"!\",\n",
    "    \"áŠ\",\n",
    "    \"ð–º˜\",\n",
    "    \"â‡\",\n",
    "    \"ð‘—Œ\",\n",
    "    \"ð‘‘‹\",\n",
    "    \"ð–­„\",\n",
    "    \"á­Ÿ\",\n",
    "    \"ð‘…\",\n",
    "    \"ð‘™\",\n",
    "    \"â¸¼\",\n",
    "    \"ê©\",\n",
    "    \"ð‘—‹\",\n",
    "    \"ã€‚\",\n",
    "    \"ê§ˆ\",\n",
    "    \"ê«±\",\n",
    "    \"ð‘œ½\",\n",
    "    \"ð½–\",\n",
    "    \"ð‘‚¿\",\n",
    "    \"á™®\",\n",
    "    \"áŸ”\",\n",
    "    \"ê›·\",\n",
    "    \"\\U00010f89\",\n",
    "    \"áŸš\",\n",
    "    \"á¥„\",\n",
    "    \"ð‘—•\",\n",
    "    \"ð‘—Ž\",\n",
    "    \"áªª\",\n",
    "    \"á­š\",\n",
    "    \"à ½\",\n",
    "    \"ð‘‡ž\",\n",
    "    \"ð‘—Š\",\n",
    "    \"ð½˜\",\n",
    "    \"\\u2e53\",\n",
    "    \"ð‘—”\",\n",
    "    \"ð–©¯\",\n",
    "    \"ð‘‡\",\n",
    "    \"ð‘»·\",\n",
    "    \"ð½•\",\n",
    "    \"ð‘©ƒ\",\n",
    "    \"à¥¤\",\n",
    "    \"ð‘—‚\",\n",
    "    \"ð‘‡†\",\n",
    "    \"ð‘ˆ\",\n",
    "    \"á‹\",\n",
    "    \"á±¾\",\n",
    "    \"ð‘±\",\n",
    "    \"ê˜\",\n",
    "    \"Ü\",\n",
    "    \"áœ¶\",\n",
    "    \"â€¼\",\n",
    "    \"ð‘ˆ»\",\n",
    "    \"â€½\",\n",
    "    \"áª«\",\n",
    "    \"ï¹–\",\n",
    "    \"ð‘‘Œ\",\n",
    "    \"ð‘ˆ¼\",\n",
    "    \"\\U00010f87\",\n",
    "    \"ð‘—\",\n",
    "    \"áŸ™\",\n",
    "    \"á°»\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "232bed2e4faae257",
   "metadata": {},
   "source": [
    "!pip install pythainlp pyyaml"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41ed251aea5e8889",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def separate_words(lang: str, text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "        Separates string (text) in lang `lang` into multiple words\n",
    "    :param lang: \n",
    "    :param text: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if lang == \"tha_Thai\":\n",
    "        from pythainlp.tokenize import word_tokenize as th_word_tokenize\n",
    "\n",
    "        tokens = th_word_tokenize(text, keep_whitespace=False, engine=\"newmm-safe\")\n",
    "        return [el.strip() for el in tokens if len(el.strip()) > 0]\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def filter_docs(lang: str, docs: list[Document], filter_config: dict) -> tuple[list[Document], list[Document]]:\n",
    "    \"\"\"\n",
    "        Filters `docs` (that are in `lang` language) using some heuristic filters. Thresholds are defined in `filter_config`.\n",
    "        Returns (kept documents, removed documents)\n",
    "    :param lang: \n",
    "    :param docs: \n",
    "    :param filter_config: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    filtered, removed = [], []\n",
    "    for doc in docs:\n",
    "        lines = doc.text.split(\"\\n\")\n",
    "        lines = [line for line in lines if line.strip() != \"\"]\n",
    "        words = separate_words(lang, doc.text)\n",
    "        \n",
    "        if len(lines) == 0:\n",
    "            doc.remove_reason = \"empty\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "        \n",
    "        avg_n_words = np.mean([len(w) for w in words])\n",
    "        if avg_n_words <= filter_config[\"min_avg_word_length\"]:\n",
    "            doc.remove_reason = \"min_avg_word_length\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "\n",
    "        if avg_n_words >= filter_config[\"max_avg_word_length\"]:\n",
    "            doc.remove_reason = \"max_avg_word_length\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "            \n",
    "        if sum(w in filter_config[\"stopwords\"] for w in words) < 2:\n",
    "            doc.remove_reason = \"stopwords\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "            \n",
    "        ratio = sum(1 for line in lines if line.endswith(TERMINAL_PUNCTUATION)) / len(lines)\n",
    "        if ratio <= filter_config[\"line_punct_thr\"]:\n",
    "            doc.remove_reason = \"line_punct_thr\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "        \n",
    "        new_line = doc.text.count(\"\\n\")\n",
    "        if new_line / len(words) >= filter_config[\"new_line_ratio\"]:\n",
    "            doc.remove_reason = \"new_line_ratio\"\n",
    "            removed.append(doc)\n",
    "            continue\n",
    "        \n",
    "        # all good\n",
    "        filtered.append(doc)\n",
    "    return filtered, removed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c360c5a639a207",
   "metadata": {},
   "source": [
    "Load thresholds from config"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb16d0a4041ecb55",
   "metadata": {},
   "source": [
    "import yaml\n",
    "\n",
    "filter_configs = {}\n",
    "for lang in LANGS:\n",
    "    with open(f\"filter-configs/{lang}.yml\") as f:\n",
    "        filter_configs[lang] = yaml.safe_load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94efdeadb501c7de",
   "metadata": {},
   "source": [
    "filter_configs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e30e8bb439ac7c3",
   "metadata": {},
   "source": [
    "Actually filter"
   ]
  },
  {
   "cell_type": "code",
   "id": "6e83849a9628daf8",
   "metadata": {},
   "source": [
    "result = {}\n",
    "for lang, docs in docs_per_lang.items():\n",
    "    filter_config = filter_configs[lang]\n",
    "    filtered, removed = filter_docs(lang, docs, filter_config)\n",
    "    result[lang] = (filtered, removed)\n",
    "    save_documents(filtered, Path(f\"filter-results/{lang}/filtered\"))\n",
    "    save_documents(removed, Path(f\"filter-results/{lang}/removed\"))\n",
    "    print(f\"Filtered {len(docs)} {lang} documents. Kept: {len(filtered)} | Removed: {len(removed)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c28f729ce74b99d0",
   "metadata": {},
   "source": [
    "Some stats on the filtering"
   ]
  },
  {
   "cell_type": "code",
   "id": "6fdaa99f38f0f087",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "for lang in LANGS:\n",
    "    reasons = Counter()\n",
    "    for removed_doc in result[lang][1]:\n",
    "        reasons[removed_doc.remove_reason] += 1\n",
    "    print(lang, \"remove reasons:\", reasons)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
